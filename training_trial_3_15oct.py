# -*- coding: utf-8 -*-
"""training trial 3 15oct.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dHTNv_zH3k5JFpXRVV-t6EkaU3jFskp7
"""

from google.colab import drive
drive.mount('/content/drive')

# Upgrade TensorFlow and Keras
!pip install tensorflow==2.17.0 keras==3.5.0

import os
import random
import numpy as np
from PIL import Image

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models, optimizers, losses, callbacks
from tensorflow.keras.applications.xception import preprocess_input

from sklearn.model_selection import train_test_split
import json

print(tf.__version__)
print(tf.keras.__version__)

# Parameters
TIME_STEPS = 30  # Number of frames per video
SKIP_FRAMES = 20  # Number of frames to skip
HEIGHT, WIDTH = 299, 299
LSTM_UNITS = 256
NUM_CLASSES = 2  # Real and Fake
DROPOUT_RATE = 0.5
TOTAL_EPOCHS = 15
BATCH_SIZE = 12

# Load JSON file
with open('/content/drive/MyDrive/Dataset DDM/Processed_FF+/Copy of dataset_splits.json', 'r') as f:
    data = json.load(f)

# Extracting training, validation, and test sets
train_data = data['training_set']
val_data = data['validation_set']
test_data = data['test_set']

# Create a dictionary to store labels and paths
def create_labels_dict(dataset):
    labels = {}
    for entry in dataset:
        folder = entry['folder']   # We don't necessarily need 'folder' beyond reference
        label = 0 if entry['label'] == 'real' else 1  # Convert label to 0 or 1
        path = entry['path']       # Full path to the video folder
        labels[folder] = (label, path)  # Store label and path using folder as key
    return labels

# Create labels and paths for each set
train_labels = create_labels_dict(train_data)
val_labels = create_labels_dict(val_data)
test_labels = create_labels_dict(test_data)

# Get the list of folder names (for reference) in each dataset
train_IDs = list(train_labels.keys())
val_IDs = list(val_labels.keys())
test_IDs = list(test_labels.keys())

# Data preprocessing function
def preprocess_image(image):
    image = np.array(image)
    image = preprocess_input(image)  # Normalize for the model
    return image

# Define VideoDataset class to handle paths and labels from JSON
class VideoDataset(tf.keras.utils.Sequence):
    def __init__(self, list_IDs, labels, batch_size, num_frames=30, skip_frames=20, shuffle=True, **kwargs):
        super().__init__(**kwargs)
        self.list_IDs = list_IDs      # Folder names (for referencing)
        self.labels = labels          # Dictionary with paths and labels
        self.batch_size = batch_size
        self.num_frames = num_frames
        self.skip_frames = skip_frames
        self.shuffle = shuffle
        self.on_epoch_end()

    def __len__(self):
        return int(np.ceil(len(self.list_IDs) / self.batch_size))

    def __getitem__(self, index):
        # Generate indexes of the batch
        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]
        batch_IDs = [self.list_IDs[k] for k in indexes]  # Get folder names for batch
        X, y = self.__data_generation(batch_IDs)
        return X, y

    def on_epoch_end(self):
        # Shuffle indexes after each epoch
        self.indexes = np.arange(len(self.list_IDs))
        if self.shuffle:
            np.random.shuffle(self.indexes)

    def __data_generation(self, batch_IDs):
      X = []
      y = []

      for ID in batch_IDs:
          label, video_dir = self.labels[ID]  # Get label and path for this folder

        # Fetch frames from the directory
          frames = sorted(os.listdir(video_dir))

        # If folder is empty, skip this video
          if len(frames) == 0:
              print(f"No frames found in directory {video_dir}, skipping.")
              continue  # Skip if no frames found

        # Apply skip_frames and num_frames logic
          frames = frames[self.skip_frames:self.skip_frames + self.num_frames]

        # Ensure we don't try to access frames[-1] if frames is empty
          if len(frames) > 0 and len(frames) < self.num_frames:
              frames += [frames[-1]] * (self.num_frames - len(frames))  # Pad with the last frame
          elif len(frames) == 0:
            # If no frames available after applying skip_frames, skip this video
              print(f"Not enough frames after skipping in {video_dir}, skipping.")
              continue

          frames_array = []
          for frame_name in frames:
              frame_path = os.path.join(video_dir, frame_name)  # Use the path to frames
              img = Image.open(frame_path).convert('RGB')
              img = preprocess_image(img)
              frames_array.append(img)

          X.append(frames_array)
          y.append(label)

      X = np.array(X)  # Shape: (batch_size, num_frames, H, W, C)
      y = np.array(y)
      return X, y

# Create datasets
train_dataset = VideoDataset(train_IDs, train_labels, BATCH_SIZE, num_frames=TIME_STEPS, skip_frames=SKIP_FRAMES, shuffle=True)
val_dataset = VideoDataset(val_IDs, val_labels, BATCH_SIZE, num_frames=TIME_STEPS, skip_frames=SKIP_FRAMES, shuffle=False)
test_dataset = VideoDataset(test_IDs, test_labels, BATCH_SIZE, num_frames=TIME_STEPS, skip_frames=SKIP_FRAMES, shuffle=False)

# Build the model
def build_model(lstm_hidden_size=256, num_classes=2, dropout_rate=0.5):
    inputs = layers.Input(shape=(TIME_STEPS, HEIGHT, WIDTH, 3))

    base_model = keras.applications.Xception(weights='imagenet', include_top=False, pooling='avg')
    for layer in base_model.layers:
        layer.trainable = False  # Freeze base model layers initially

    x = layers.TimeDistributed(base_model)(inputs)
    x = layers.LSTM(lstm_hidden_size)(x)
    x = layers.Dropout(dropout_rate)(x)
    outputs = layers.Dense(num_classes, activation='softmax')(x)

    model = keras.Model(inputs, outputs)
    return model

model = build_model(lstm_hidden_size=LSTM_UNITS, num_classes=NUM_CLASSES, dropout_rate=DROPOUT_RATE)
model.summary()

# model.load_weights('/content/drive/MyDrive/Dataset DDM/Saved models_trial 2 1385 vids each/best_model_phase1.keras')
# Compile the model
model.compile(optimizer=optimizers.Adam(learning_rate=1e-3),
              loss=losses.SparseCategoricalCrossentropy(),
              metrics=['accuracy'])
model_save_dir='/content/drive/MyDrive/Dataset DDM/Saved models_trial 2 1385 vids each'

# weights_path = os.path.join(model_save_dir, 'best_model_phase1.keras')


# call saved model for next session
model = keras.models.load_model('/content/drive/MyDrive/Dataset DDM/Saved models_trial 2 1385 vids each/epoch3_best_model_phase1.keras')
model.summary()
# Callbacks
checkpoint_phase1 = callbacks.ModelCheckpoint(
    filepath=os.path.join(model_save_dir, 'best_model_phase1.keras'),
    monitor='accuracy',
    save_best_only=True,
    save_weights_only=False,
    verbose=1
)

early_stopping_phase1 = callbacks.EarlyStopping(
    monitor='val_loss',
    patience=3,
    verbose=1
)

# Training Phase 1
num_epochs_phase1 = 4  # Set the desired number of epochs
# weights_path = os.path.join(model_save_dir, 'best_model_phase1.keras')
# model.load_weights(weights_path)
history_phase1 = model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=num_epochs_phase1,
    callbacks=[checkpoint_phase1, early_stopping_phase1],
    verbose=1
)

# # Unfreeze base model layers for fine-tuning
# for layer in model.layers[1].layer.layers:
#     layer.trainable = True
# Unfreeze the top 20 layers of the base model
for layer in model.layers[1].layer.layers[-40:]:
    layer.trainable = True

# Re-compile the model with a lower learning rate
model.compile(optimizer=optimizers.Adam(learning_rate=1e-5),
              loss=losses.SparseCategoricalCrossentropy(),
              metrics=['accuracy'])

# Callbacks for Phase 2
checkpoint_phase2 = callbacks.ModelCheckpoint(
    filepath=os.path.join(model_save_dir, 'best_model_phase2.keras'),
    monitor='val_loss',
    save_best_only=True,
    verbose=1
)

early_stopping_phase2 = callbacks.EarlyStopping(
    monitor='val_loss',
    patience=5,
    verbose=1
)
# Training Phase 2
# Reduced BATCH_SIZE further to mitigate OOM error
# BATCH_SIZE = 2
num_epochs_phase2 = 25

# Update train_dataset and val_dataset with the new batch size
train_dataset.batch_size = BATCH_SIZE
val_dataset.batch_size = BATCH_SIZE


history_phase2 = model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=num_epochs_phase2 + num_epochs_phase1,
    callbacks=[checkpoint_phase2, early_stopping_phase2],
    verbose=1,
    initial_epoch=num_epochs_phase1
)

# Load the best model from Phase 2
model.load_weights(os.path.join(model_save_dir, 'best_model_phase2.keras'))

# Load the best model from Phase 2
model.load_weights(os.path.join(model_save_dir, 'Phase 2_Epoch4.keras'))
val_loss, val_acc = model.evaluate(val_dataset, verbose=1)
print(f'Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}')